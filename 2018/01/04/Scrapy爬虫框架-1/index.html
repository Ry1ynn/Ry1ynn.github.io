<!DOCTYPE html>



  



<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=5.1.4">






  <meta name="keywords" content="python,爬虫,scrapy,">










<meta name="description" content="Scrapy爬虫框架_01 目录 配置安装 scrapy简介 入门案例 Scrapy Shell Item Pipeline    00-配置安装 Windows安装：">
<meta name="keywords" content="python,爬虫,scrapy">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy爬虫框架-1">
<meta property="og:url" content="http://21guns.top/2018/01/04/Scrapy爬虫框架-1/index.html">
<meta property="og:site_name" content="Ry1ynn&#39;s blogs">
<meta property="og:description" content="Scrapy爬虫框架_01 目录 配置安装 scrapy简介 入门案例 Scrapy Shell Item Pipeline    00-配置安装 Windows安装：">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://ot12pfxkm.bkt.clouddn.com/17-12-22/38554764.jpg">
<meta property="og:image" content="http://ot12pfxkm.bkt.clouddn.com/17-12-13/56845982.jpg">
<meta property="og:image" content="http://ot12pfxkm.bkt.clouddn.com/17-12-13/26663513.jpg">
<meta property="og:image" content="http://ot12pfxkm.bkt.clouddn.com/17-12-13/85337946.jpg">
<meta property="og:updated_time" content="2019-02-18T09:14:27.604Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scrapy爬虫框架-1">
<meta name="twitter:description" content="Scrapy爬虫框架_01 目录 配置安装 scrapy简介 入门案例 Scrapy Shell Item Pipeline    00-配置安装 Windows安装：">
<meta name="twitter:image" content="http://ot12pfxkm.bkt.clouddn.com/17-12-22/38554764.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://21guns.top/2018/01/04/Scrapy爬虫框架-1/">





  <title>Scrapy爬虫框架-1 | Ry1ynn's blogs</title>
  










</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Ry1ynn's blogs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://21guns.top/2018/01/04/Scrapy爬虫框架-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ry1ynn">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ry1ynn's blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Scrapy爬虫框架-1</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-04T17:06:11+08:00">
                2018-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/爬虫/" itemprop="url" rel="index">
                    <span itemprop="name">爬虫</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Scrapy爬虫框架-01"><a href="#Scrapy爬虫框架-01" class="headerlink" title="Scrapy爬虫框架_01"></a>Scrapy爬虫框架_01</h1><ul>
<li>目录<ul>
<li>配置安装</li>
<li>scrapy简介</li>
<li>入门案例</li>
<li>Scrapy Shell</li>
<li>Item Pipeline</li>
</ul>
</li>
</ul>
<h2 id="00-配置安装"><a href="#00-配置安装" class="headerlink" title="00-配置安装"></a>00-配置安装</h2><ul>
<li>Windows安装：<a id="more"></a><ul>
<li>Python 2 / 3</li>
<li>升级pip版本：<code>pip install --upgrade pip</code></li>
<li>通过pip 安装 Scrapy 框架<code>pip install Scrapy</code></li>
</ul>
</li>
<li>Ubuntu安装<ul>
<li>需9.10或以上版本</li>
<li>Python 2 / 3</li>
<li>安装非Python的依赖 <code>sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev</code></li>
<li>通过pip 安装 Scrapy 框架 <code>sudo pip install scrapy</code></li>
</ul>
</li>
<li>检验<ul>
<li>命令行下键入<code>scrapy</code></li>
<li>输出相关信息则安装成功</li>
</ul>
</li>
</ul>
<h2 id="01-scrapy简介"><a href="#01-scrapy简介" class="headerlink" title="01-scrapy简介"></a>01-scrapy简介</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1-简介"></a>1.1-简介</h3><ul>
<li>Scrapy是用纯Python实现一个为了爬取网站数据、提取结构性数据而编写的应用框架，用途非常广泛。</li>
<li>框架的力量，用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。</li>
<li>Scrapy 使用了 Twisted<code>[&#39;twɪstɪd]</code>(其主要对手是Tornado)异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。</li>
</ul>
<h3 id="1-2-Scrapy架构图"><a href="#1-2-Scrapy架构图" class="headerlink" title="1.2-Scrapy架构图"></a>1.2-Scrapy架构图</h3><blockquote>
<p>绿线为数据流向</p>
</blockquote>
<p><img src="http://ot12pfxkm.bkt.clouddn.com/17-12-22/38554764.jpg" alt></p>
<ul>
<li><code>Scrapy Engine(引擎)</code>: 负责<code>Spider</code>、<code>ItemPipeline</code>、<code>Downloader</code>、<code>Scheduler</code>中间的通讯，信号、数据传递等。</li>
<li><code>Scheduler(调度器)</code>: 它负责接受<code>引擎</code>发送过来的Request请求，并按照一定的方式进行整理排列，入队，当<code>引擎</code>需要时，交还给<code>引擎</code>。</li>
<li><code>Downloader（下载器）</code>：负责下载<code>Scrapy Engine(引擎)</code>发送的所有Requests请求，并将其获取到的Responses交还给<code>Scrapy Engine(引擎)</code>，由<code>引擎</code>交给<code>Spider</code>来处理，</li>
<li><code>Spider（爬虫）</code>：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给<code>引擎</code>，再次进入<code>Scheduler(调度器)</code>，</li>
<li><code>Item Pipeline(管道)</code>：它负责处理<code>Spider</code>中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.</li>
<li><code>Downloader Middlewares（下载中间件）</code>：你可以当作是一个可以自定义扩展下载功能的组件。</li>
<li><code>Spider Middlewares（Spider中间件）</code>：你可以理解为是一个可以自定扩展和操作<code>引擎</code>和<code>Spider</code>中间<code>通信</code>的功能组件（比如进入<code>Spider</code>的Responses;和从<code>Spider</code>出去的Requests）</li>
</ul>
<h3 id="1-3-Scrapy的运作流程"><a href="#1-3-Scrapy的运作流程" class="headerlink" title="1.3-Scrapy的运作流程"></a>1.3-Scrapy的运作流程</h3><p>代码写好，程序开始运行…</p>
<ol>
<li><code>引擎</code>：Hi！<code>Spider</code>, 你要处理哪一个网站？</li>
<li><code>Spider</code>：老大要我处理xxxx.com。</li>
<li><code>引擎</code>：你把第一个需要处理的URL给我吧。</li>
<li><code>Spider</code>：给你，第一个URL是xxxxxxx.com。</li>
<li><code>引擎</code>：Hi！<code>调度器</code>，我这有request请求你帮我排序入队一下。</li>
<li><code>调度器</code>：好的，正在处理你等一下。</li>
<li><code>引擎</code>：Hi！<code>调度器</code>，把你处理好的request请求给我。</li>
<li><code>调度器</code>：给你，这是我处理好的request</li>
<li><code>引擎</code>：Hi！下载器，你按照老大的<code>下载中间件</code>的设置帮我下载一下这个request请求</li>
<li><code>下载器</code>：好的！给你，这是下载好的东西。（如果失败：sorry，这个request下载失败了。然后<code>引擎</code>告诉<code>调度器</code>，这个request下载失败了，你记录一下，我们待会儿再下载）</li>
<li><code>引擎</code>：Hi！<code>Spider</code>，这是下载好的东西，并且已经按照老大的<code>下载中间件</code>处理过了，你自己处理一下（注意！这儿responses默认是交给<code>def parse()</code>这个函数处理的）</li>
<li><code>Spider</code>：（处理完毕数据之后对于需要跟进的URL），Hi！<code>引擎</code>，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。</li>
<li><code>引擎</code>：Hi ！<code>管道</code> 我这儿有个item你帮我处理一下！<code>调度器</code>！这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完老大需要全部信息。</li>
<li><code>管道`</code>调度器`：好的，现在就做！</li>
</ol>
<ul>
<li>注意：<ul>
<li>只有当调度器中不存在任何request了，整个程序才会停止，</li>
<li>也就是说，对于下载失败的URL，Scrapy也会重新下载。</li>
</ul>
</li>
</ul>
<h3 id="1-4-制作scrapy爬虫四步骤"><a href="#1-4-制作scrapy爬虫四步骤" class="headerlink" title="1.4-制作scrapy爬虫四步骤"></a>1.4-制作scrapy爬虫四步骤</h3><ul>
<li>新建项目 (scrapy startproject xxx)：新建一个新的爬虫项目</li>
<li>明确目标 （编写items.py）：明确你想要抓取的目标</li>
<li>制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页</li>
<li>存储内容 （pipelines.py）：设计管道存储爬取内容</li>
</ul>
<h2 id="02-入门案例"><a href="#02-入门案例" class="headerlink" title="02-入门案例"></a>02-入门案例</h2><h3 id="2-1-新建项目"><a href="#2-1-新建项目" class="headerlink" title="2.1-新建项目"></a>2.1-新建项目</h3><ul>
<li><p>命令格式：<code>scrapy startproject mySpider</code></p>
</li>
<li><p>mySpider 为项目名称,会创建一个 mySpider 文件夹</p>
</li>
<li><p>项目目录结构，如下：</p>
<p><img src="http://ot12pfxkm.bkt.clouddn.com/17-12-13/56845982.jpg" alt></p>
<ul>
<li>各文件作用简介</li>
</ul>
<blockquote>
<p>scrapy.cfg ：项目的配置文件</p>
<p>mySpider/ ：项目的Python模块，将会从这里引用代码</p>
<p>mySpider/items.py ：项目的目标文件</p>
<p>mySpider/pipelines.py ：项目的管道文件</p>
<p>mySpider/settings.py ：项目的设置文件</p>
<p>mySpider/spiders/ ：存储爬虫代码目录</p>
</blockquote>
</li>
</ul>
<h3 id="2-2-明确目标"><a href="#2-2-明确目标" class="headerlink" title="2.2-明确目标"></a>2.2-明确目标</h3><blockquote>
<p>假设抓取某网站，所有老师的姓名、性别、年龄、个人信息等</p>
</blockquote>
<ol>
<li>打开mySpider目录下的items.py</li>
<li>Item 定义结构化数据字段，用来保存爬取到的数据，有点像Python中的dict，但是提供了一些额外的保护减少错误。</li>
<li>可以通过创建一个 scrapy.Item 类， 并且定义类型为 scrapy.Field的类属性来定义一个Item（可以理解成类似于ORM的映射关系）。</li>
<li>接下来，创建一个ItcastItem 类，和构建item模型（model）。</li>
</ol>
<ul>
<li><p>code：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ItcastItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    level = scrapy.Field()</span><br><span class="line">    info = scrapy.Field()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-3-编写爬虫"><a href="#2-3-编写爬虫" class="headerlink" title="2.3-编写爬虫"></a>2.3-编写爬虫</h3><ul>
<li>爬虫功能分两步<ul>
<li>爬数据</li>
<li>取数据</li>
</ul>
</li>
</ul>
<h4 id="2-3-1-爬数据"><a href="#2-3-1-爬数据" class="headerlink" title="2.3.1-爬数据"></a>2.3.1-爬数据</h4><ol>
<li><p>在当前目录下输入命令，将在<code>mySpider/spider</code>目录下创建一个名为<code>itcast</code>的爬虫，并指定爬取域的范围，如<code>scrapy genspider itcast &quot;itcast.cn&quot;</code></p>
</li>
<li><p>打开 mySpider/spider目录里的 itcast.py，默认增加了下列代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ItcastSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"itcast"</span></span><br><span class="line">    allowed_domains = [<span class="string">"itcast.cn"</span>]</span><br><span class="line">    start_urls = (</span><br><span class="line">        <span class="string">'http://www.itcast.cn/'</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<blockquote>
<p>其实也可以由我们自行创建itcast.py并编写上面的代码，只不过使用命令可以免去编写固定代码的麻烦</p>
</blockquote>
<ul>
<li><p>要建立一个Spider， 你必须用scrapy.Spider类创建一个子类，并确定了三个强制的属性 和 一个方法。</p>
<ul>
<li><code>name = &quot;&quot;</code> ：这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。</li>
<li><code>allow_domains = []</code> 是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。</li>
<li><code>start_urls = ()</code> ：爬取的URL元祖/列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。</li>
<li><code>parse(self, response)</code> ：解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下：<ul>
<li>负责解析返回的网页数据(response.body)，提取结构化数据(生成item)</li>
<li>生成需要下一页的URL请求。</li>
</ul>
</li>
</ul>
</li>
<li><p>将start_urls的值修改为需要爬取的第一个url</p>
<p><code>start_urls = (&quot;http://www.itcast.cn/channel/teacher.shtml&quot;,)</code></p>
</li>
<li><p>修改parse()方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"teacher.html"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(response.text)</span><br></pre></td></tr></table></figure>
</li>
<li><p>运行以测试</p>
<ul>
<li><p>在mySpider目录下执行：<code>scrapy crawl itcast</code></p>
<ul>
<li>是的，就是 itcast，看上面代码，它是 ItcastSpider 类的 name 属性，也就是使用 <code>scrapy genspider</code>命令的爬虫名。</li>
<li>一个Scrapy爬虫项目里，可以存在多个爬虫。各个爬虫在执行时，就是按照 name 属性来区分。</li>
</ul>
</li>
<li><p>运行之后，如果打印的日志出现 <code>[scrapy] INFO: Spider closed (finished)</code>，代表执行完成。 之后当前文件夹中就出现了一个 teacher.html 文件，里面就是我们刚刚要爬取的网页的全部源代码信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意，Python2.x默认编码环境是ASCII，当和取回的数据编码格式不一致时，可能会造成乱码；</span></span><br><span class="line"><span class="comment"># 我们可以指定保存内容的编码格式，一般情况下，我们可以在代码最上方添加：</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> sys</span><br><span class="line">    reload(sys)</span><br><span class="line">    sys.setdefaultencoding(<span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这三行代码是Python2.x里解决中文编码的万能钥匙，经过这么多年的吐槽后Python3学乖了，默认编码是Unicode了...(祝大家早日拥抱Python3)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h4 id="2-3-2-取数据"><a href="#2-3-2-取数据" class="headerlink" title="2.3.2-取数据"></a>2.3.2-取数据</h4><ul>
<li><p>爬取整个网页完毕，接下来的就是的取过程了，首先观察页面源码：</p>
<p><img src="http://ot12pfxkm.bkt.clouddn.com/17-12-13/26663513.jpg" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class=&quot;li_txt&quot;&gt;</span><br><span class="line">    &lt;h3&gt;  xxx  &lt;/h3&gt;</span><br><span class="line">    &lt;h4&gt; xxxxx &lt;/h4&gt;</span><br><span class="line">    &lt;p&gt; xxxxxxxx &lt;/p&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>是不是一目了然？直接上XPath开始提取数据吧。</p>
</li>
</ul>
<h5 id="xpath提取数据"><a href="#xpath提取数据" class="headerlink" title="xpath提取数据"></a>xpath提取数据</h5><ul>
<li><p>之前在mySpider/items.py 里定义了一个ItcastItem类。</p>
<p> 这里引入进来，<code>from mySpider.items import ItcastItem</code></p>
</li>
<li><p>然后将我们得到的数据封装到一个 <code>ItcastItem</code> 对象中，可以保存每个老师的属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mySpider.items <span class="keyword">import</span> ItcastItem</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment">#open("teacher.html","wb").write(response.body).close()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存放老师信息的集合</span></span><br><span class="line">    items = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> response.xpath(<span class="string">"//div[@class='li_txt']"</span>):</span><br><span class="line">        <span class="comment"># 将我们得到的数据封装到一个 `ItcastItem` 对象</span></span><br><span class="line">        item = ItcastItem()</span><br><span class="line">        <span class="comment">#extract()方法返回的都是unicode字符串</span></span><br><span class="line">        name = each.xpath(<span class="string">"h3/text()"</span>).extract()</span><br><span class="line">        title = each.xpath(<span class="string">"h4/text()"</span>).extract()</span><br><span class="line">        info = each.xpath(<span class="string">"p/text()"</span>).extract()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#xpath返回的是包含一个元素的列表</span></span><br><span class="line">        item[<span class="string">'name'</span>] = name[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">'title'</span>] = title[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">'info'</span>] = info[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        items.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 直接返回最后数据</span></span><br><span class="line">    <span class="keyword">return</span> items</span><br></pre></td></tr></table></figure>
</li>
<li><p>我们暂时先不处理管道，后面会详细介绍。</p>
</li>
</ul>
<h3 id="2-4-保存数据"><a href="#2-4-保存数据" class="headerlink" title="2.4-保存数据"></a>2.4-保存数据</h3><ul>
<li><p>scrapy保存信息的最简单的方法主要有四种，-o 输出指定格式的文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> json格式，默认为Unicode编码</span><br><span class="line">scrapy crawl itcast -o teachers.json</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> json lines格式，默认为Unicode编码</span><br><span class="line">scrapy crawl itcast -o teachers.jsonl</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> csv 逗号表达式，可用Excel打开</span><br><span class="line">scrapy crawl itcast -o teachers.csv</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> xml格式</span><br><span class="line">scrapy crawl itcast -o teachers.xml</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="03-Scrapy-Shell"><a href="#03-Scrapy-Shell" class="headerlink" title="03-Scrapy Shell"></a>03-Scrapy Shell</h2><h3 id="3-1-简介"><a href="#3-1-简介" class="headerlink" title="3.1-简介"></a>3.1-简介</h3><ul>
<li>Scrapy终端是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath或CSS表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据。</li>
<li>如果安装了 IPython ，Scrapy终端将使用 IPython (替代标准Python终端)。 IPython 终端与其他相比更为强大，提供智能的自动补全，高亮输出，及其他特性。（推荐安装IPython）</li>
<li>以后做数据提取的时候，可以把现在Scrapy Shell中测试，测试通过后再应用到代码中。</li>
</ul>
<h3 id="3-2-启动"><a href="#3-2-启动" class="headerlink" title="3.2-启动"></a>3.2-启动</h3><ul>
<li><p>进入项目的根目录，执行下列命令来启动shell</p>
<p><code>scrapy shell &quot;http://www.itcast.cn/channel/teacher.shtml&quot;</code></p>
</li>
<li><p>启动后效果如下：</p>
<p><img src="http://ot12pfxkm.bkt.clouddn.com/17-12-13/85337946.jpg" alt></p>
</li>
<li><p>说明</p>
<ul>
<li>Scrapy Shell根据下载的页面会自动创建一些方便使用的对象，例如 Response 对象，以及 Selector 对象 (对HTML及XML内容)。</li>
<li>当shell载入后，将得到一个包含response数据的本地 response 变量，输入 response.body将输出response的包体，输出 response.headers 可以看到response的包头。</li>
<li>输入 <code>response.selector</code> 时， 将获取到一个response 初始化的类 Selector 的对象，此时可以通过使用 <code>response.selector.xpath()</code>或<code>response.selector.css()</code> 来对 response 进行查询。</li>
<li>Scrapy也提供了一些快捷方式, 例如 <code>response.xpath()</code>或<code>response.css()</code>同样可以生效（如之前的案例）。</li>
</ul>
</li>
</ul>
<h3 id="3-3-Selector选择器"><a href="#3-3-Selector选择器" class="headerlink" title="3.3-Selector选择器"></a>3.3-Selector选择器</h3><ul>
<li><p>Scrapy Selectors 内置 XPath 和 CSS Selector 表达式机制</p>
</li>
<li><p>Selector有四个基本的方法，最常用的还是xpath:</p>
<ul>
<li>xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表</li>
<li>extract(): 序列化该节点为Unicode字符串并返回list</li>
<li>css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表，语法同 BeautifulSoup4</li>
<li>re(): 根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表</li>
</ul>
</li>
<li><p>XPath表达式的例子及对应的含义:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/html/head/title: 选择&lt;HTML&gt;文档中 &lt;head&gt; 标签内的 &lt;title&gt; 元素</span><br><span class="line">/html/head/title/text(): 选择上面提到的 &lt;title&gt; 元素的文字</span><br><span class="line">//td: 选择所有的 &lt;td&gt; 元素</span><br><span class="line">//div[@class="mine"]: 选择所有具有 class="mine" 属性的 div 元素</span><br></pre></td></tr></table></figure>
</li>
<li><p>demo：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动</span></span><br><span class="line">scrapy shell <span class="string">"http://hr.tencent.com/position.php?&amp;start=0#a"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回 xpath选择器对象列表</span></span><br><span class="line">response.xpath(<span class="string">'//title'</span>)</span><br><span class="line">[&lt;Selector xpath=<span class="string">'//title'</span> data=<span class="string">u'&lt;title&gt;\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058&lt;/title'</span>&gt;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 extract()方法返回 Unicode字符串列表</span></span><br><span class="line">response.xpath(<span class="string">'//title'</span>).extract()</span><br><span class="line">[<span class="string">u'&lt;title&gt;\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058&lt;/title&gt;'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印列表第一个元素，终端编码格式显示</span></span><br><span class="line"><span class="keyword">print</span> response.xpath(<span class="string">'//title'</span>).extract()[<span class="number">0</span>]</span><br><span class="line">&lt;title&gt;职位搜索 | 社会招聘 | Tencent 腾讯招聘&lt;/title&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回 xpath选择器对象列表</span></span><br><span class="line">response.xpath(<span class="string">'//title/text()'</span>)</span><br><span class="line">&lt;Selector xpath=<span class="string">'//title/text()'</span> data=<span class="string">u'\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058'</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回列表第一个元素的Unicode字符串</span></span><br><span class="line">response.xpath(<span class="string">'//title/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line"><span class="string">u'\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按终端编码格式显示</span></span><br><span class="line"><span class="keyword">print</span> response.xpath(<span class="string">'//title/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">职位搜索 | 社会招聘 | Tencent 腾讯招聘</span><br><span class="line"></span><br><span class="line">response.xpath(<span class="string">'//*[@class="even"]'</span>)</span><br><span class="line">职位名称:</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> site[<span class="number">0</span>].xpath(<span class="string">'./td[1]/a/text()'</span>).extract()[<span class="number">0</span>]</span><br><span class="line">TEG15-运营开发工程师（深圳）</span><br><span class="line">职位名称详情页:</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> site[<span class="number">0</span>].xpath(<span class="string">'./td[1]/a/@href'</span>).extract()[<span class="number">0</span>]</span><br><span class="line">position_detail.php?id=20744&amp;keywords=&amp;tid=0&amp;lid=0</span><br><span class="line">职位类别:</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> site[<span class="number">0</span>].xpath(<span class="string">'./td[2]/text()'</span>).extract()[<span class="number">0</span>]</span><br><span class="line">技术类</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="04-Item-Pipeline"><a href="#04-Item-Pipeline" class="headerlink" title="04-Item Pipeline"></a>04-Item Pipeline</h2><h3 id="4-1-简介"><a href="#4-1-简介" class="headerlink" title="4.1-简介"></a>4.1-简介</h3><ul>
<li>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。</li>
<li>每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。</li>
<li>item pipeline的一些典型应用：<ul>
<li>验证爬取的数据(检查item包含某些字段，比如说name字段)</li>
<li>查重(并丢弃)</li>
<li>将爬取结果保存到文件或者数据库中</li>
</ul>
</li>
</ul>
<h3 id="4-2-编写item-pipeline"><a href="#4-2-编写item-pipeline" class="headerlink" title="4.2-编写item pipeline"></a>4.2-编写item pipeline</h3><ul>
<li><p>编写item pipeline很简单，item pipiline组件是一个独立的Python类，其中process_item()方法必须实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> something</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SomethingPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span>    </span><br><span class="line">        <span class="comment"># 可选实现，做参数初始化等</span></span><br><span class="line">        <span class="comment"># doing something</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># item (Item 对象) – 被爬取的item</span></span><br><span class="line">        <span class="comment"># spider (Spider 对象) – 爬取该item的spider</span></span><br><span class="line">        <span class="comment"># 这个方法必须实现，每个item pipeline组件都需要调用该方法，</span></span><br><span class="line">        <span class="comment"># 这个方法必须返回一个 Item 对象，被丢弃的item将不会被之后的pipeline组件所处理。</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="comment"># spider (Spider 对象) – 被开启的spider</span></span><br><span class="line">        <span class="comment"># 可选实现，当spider被开启时，这个方法被调用。</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="comment"># spider (Spider 对象) – 被关闭的spider</span></span><br><span class="line">        <span class="comment"># 可选实现，当spider被关闭时，这个方法被调用</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="4-3-完善之前案例"><a href="#4-3-完善之前案例" class="headerlink" title="4.3-完善之前案例"></a>4.3-完善之前案例</h3><h4 id="4-3-1-item写入json文件"><a href="#4-3-1-item写入json文件" class="headerlink" title="4.3.1-item写入json文件"></a>4.3.1-item写入json文件</h4><ul>
<li><p>以下pipeline将所有(从所有’spider’中)爬取到的item，存储到一个独立地items.json 文件，每行包含一个序列化为’JSON’格式的’item’。</p>
</li>
<li><p>打开 pipelines.py 文件，写入下面代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pipelines.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ItcastJsonPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'teacher.json'</span>, <span class="string">'wb'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        content = json.dumps(dict(item), ensure_ascii=<span class="keyword">False</span>) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(content)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="4-3-2-启用一个item-pipeline组件"><a href="#4-3-2-启用一个item-pipeline组件" class="headerlink" title="4.3.2-启用一个item pipeline组件"></a>4.3.2-启用一个item pipeline组件</h4><ul>
<li><p>为了启用Item Pipeline组件，必须将它的类添加到 settings.py文件</p>
</li>
<li><p>文件中ITEM_PIPELINES 字段，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Configure item pipelines</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="comment">#'mySpider.pipelines.SomePipeline': 300,</span></span><br><span class="line">    <span class="string">"mySpider.pipelines.ItcastJsonPipeline"</span>:<span class="number">300</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline</p>
</li>
<li><p>通常将这些数字定义在0-1000范围内（0-1000随意设置</p>
</li>
<li><p>数值越低，组件的优先级越高</p>
</li>
</ul>
<h4 id="4-3-3-重新启动爬虫"><a href="#4-3-3-重新启动爬虫" class="headerlink" title="4.3.3-重新启动爬虫"></a>4.3.3-重新启动爬虫</h4><ul>
<li><p>将parse()方法改为4.2中最后思考中的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mySpider.items <span class="keyword">import</span> ItcastItem</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment">#open("teacher.html","wb").write(response.body).close()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存放老师信息的集合</span></span><br><span class="line">    <span class="comment">#items = []</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> response.xpath(<span class="string">"//div[@class='li_txt']"</span>):</span><br><span class="line">        <span class="comment"># 将我们得到的数据封装到一个 `ItcastItem` 对象</span></span><br><span class="line">        item = ItcastItem()</span><br><span class="line">        <span class="comment">#extract()方法返回的都是unicode字符串</span></span><br><span class="line">        name = each.xpath(<span class="string">"h3/text()"</span>).extract()</span><br><span class="line">        title = each.xpath(<span class="string">"h4/text()"</span>).extract()</span><br><span class="line">        info = each.xpath(<span class="string">"p/text()"</span>).extract()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#xpath返回的是包含一个元素的列表</span></span><br><span class="line">        item[<span class="string">'name'</span>] = name[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">'title'</span>] = title[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">'info'</span>] = info[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#items.append(item)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#将获取的数据交给pipelines</span></span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回数据，不经过pipeline</span></span><br><span class="line">    <span class="comment">#return items</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>执行命令启动爬虫<code>scrapy crawl itcast</code></p>
</li>
<li><p>查看当前目录是否生成teacher.json</p>
</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"># python</a>
          
            <a href="/tags/爬虫/" rel="tag"># 爬虫</a>
          
            <a href="/tags/scrapy/" rel="tag"># scrapy</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/04/非结构化数据-结构化数据提取-4/" rel="next" title="非结构化数据&结构化数据提取-4">
                <i class="fa fa-chevron-left"></i> 非结构化数据&结构化数据提取-4
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/01/04/Scrapy爬虫框架-2/" rel="prev" title="Scrapy爬虫框架-2">
                Scrapy爬虫框架-2 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ry1ynn">
            
              <p class="site-author-name" itemprop="name">Ry1ynn</p>
              <p class="site-description motion-element" itemprop="description">我们的征途是星辰大海</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">131</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">59</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Ry1ynn" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:ry1ynn_pri@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/" target="_blank" title="Weibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>Weibo</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="/images/weixin.JPG" target="_blank" title="Wechat">
                      
                        <i class="fa fa-fw fa-weixin"></i>Wechat</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.t00ls.net/" title="T00ls" target="_blank">T00ls</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.freebuf.com/" title="FreeBuf" target="_blank">FreeBuf</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.sec-wiki.com/" title="SecWiki" target="_blank">SecWiki</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.seebug.org/" title="Seebug" target="_blank">Seebug</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.91ri.org/" title="91Ri" target="_blank">91Ri</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.pediy.com/" title="看雪" target="_blank">看雪</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.52pojie.cn/" title="吾爱破解" target="_blank">吾爱破解</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.anquanke.com/" title="安全客" target="_blank">安全客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.ichunqiu.com/" title="i春秋" target="_blank">i春秋</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Scrapy爬虫框架-01"><span class="nav-text">Scrapy爬虫框架_01</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#00-配置安装"><span class="nav-text">00-配置安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#01-scrapy简介"><span class="nav-text">01-scrapy简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-简介"><span class="nav-text">1.1-简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Scrapy架构图"><span class="nav-text">1.2-Scrapy架构图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Scrapy的运作流程"><span class="nav-text">1.3-Scrapy的运作流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-制作scrapy爬虫四步骤"><span class="nav-text">1.4-制作scrapy爬虫四步骤</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#02-入门案例"><span class="nav-text">02-入门案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-新建项目"><span class="nav-text">2.1-新建项目</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-明确目标"><span class="nav-text">2.2-明确目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-编写爬虫"><span class="nav-text">2.3-编写爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-1-爬数据"><span class="nav-text">2.3.1-爬数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-2-取数据"><span class="nav-text">2.3.2-取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#xpath提取数据"><span class="nav-text">xpath提取数据</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-保存数据"><span class="nav-text">2.4-保存数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#03-Scrapy-Shell"><span class="nav-text">03-Scrapy Shell</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-简介"><span class="nav-text">3.1-简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-启动"><span class="nav-text">3.2-启动</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Selector选择器"><span class="nav-text">3.3-Selector选择器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#04-Item-Pipeline"><span class="nav-text">04-Item Pipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-简介"><span class="nav-text">4.1-简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-编写item-pipeline"><span class="nav-text">4.2-编写item pipeline</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-完善之前案例"><span class="nav-text">4.3-完善之前案例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-1-item写入json文件"><span class="nav-text">4.3.1-item写入json文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-2-启用一个item-pipeline组件"><span class="nav-text">4.3.2-启用一个item pipeline组件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-3-重新启动爬虫"><span class="nav-text">4.3.3-重新启动爬虫</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2016 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ry1ynn</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  










  



  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script id="ribbon" type="text/javascript" size="300" alpha="0.5" zindex="0" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
